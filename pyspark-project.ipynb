{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import socket\n",
        "import os\n",
        "\n",
        "import mlflow\n",
        "import mlflow.spark\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression, GeneralizedLinearRegression\n",
        "from pyspark.ml.regression import RandomForestRegressor\n",
        "from pyspark.ml.tuning import ParamGridBuilder\n",
        "from pyspark.ml.tuning import CrossValidator\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "from hyperopt import fmin, tpe, Trials, hp\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "0nnkRoaNUxaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sparkSession(appName = 'MLOps'):\n",
        "    spark_master = os.environ.get('SPARK_MASTER') # \"spark://spark-master:7077\"\n",
        "    driver_host = socket.gethostbyname(socket.gethostname()) # setting driver host is important in k8s mode, ortherwise excutors cannot find diver host\n",
        "\n",
        "    spark = SparkSession \\\n",
        "        .builder \\\n",
        "        .master(spark_master)\\\n",
        "        .appName(appName) \\\n",
        "        .config(\"spark.driver.host\", driver_host) \\\n",
        "        .config('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:3.3.1') \\\n",
        "        .getOrCreate()\n",
        "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
        "\n",
        "    ACCESS_KEY = os.environ.get('AWS_ACCESS_KEY_ID')\n",
        "    SECRET_KEY = os.environ.get('AWS_SECRET_ACCESS_KEY')\n",
        "    MLFLOW_S3_ENDPOINT_URL = os.environ.get('MLFLOW_S3_ENDPOINT_URL')\n",
        "\n",
        "    hadoopConf = spark.sparkContext._jsc.hadoopConfiguration()\n",
        "    hadoopConf.set('fs.s3a.access.key', ACCESS_KEY)\n",
        "    hadoopConf.set('fs.s3a.secret.key', SECRET_KEY)\n",
        "    hadoopConf.set(\"fs.s3a.endpoint\", MLFLOW_S3_ENDPOINT_URL)\n",
        "    hadoopConf.set('fs.s3.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
        "    hadoopConf.set(\"fs.s3a.connection.ssl.enabled\", \"true\")\n",
        "    hadoopConf.set(\"fs.s3a.path.style.access\", 'true')\n",
        "\n",
        "    return spark"
      ],
      "metadata": {
        "id": "2HmifXr5UzDx"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_impute_dataframe(spark, file_uri, keep_cols, impute_cols, impute_strategy = \"median\"):\n",
        "\n",
        "    raw_df = spark.read.csv(file_uri ,header=\"true\", inferSchema=\"true\", multiLine=\"true\", escape='\"')\n",
        "    base_df = raw_df.select(*keep_cols)\n",
        "\n",
        "    from pyspark.sql.functions import col, translate, when\n",
        "    from pyspark.sql.types import IntegerType\n",
        "\n",
        "    #cast datatypes into doubles & simply remove outliers with price beyond normal ranges\n",
        "    doubles_df= base_df.withColumn(\"price\", translate(col(\"price\"), \"$,\", \"\").cast(\"double\")) \\\n",
        "                            .filter(col(\"price\") > 0).filter(col(\"minimum_nights\") <= 365)\n",
        "\n",
        "    integer_columns = [x.name for x in doubles_df.schema.fields if x.dataType == IntegerType()]\n",
        "\n",
        "    for c in integer_columns:\n",
        "        doubles_df = doubles_df.withColumn(c, col(c).cast(\"double\"))\n",
        "\n",
        "    for c in impute_cols:\n",
        "        doubles_df = doubles_df.withColumn(c + \"_na\", when(col(c).isNull(), 1.0).otherwise(0.0))\n",
        "\n",
        "    from pyspark.ml.feature import Imputer\n",
        "    imputer = Imputer(strategy=impute_strategy, inputCols=impute_cols, outputCols=impute_cols)\n",
        "    imputer_model = imputer.fit(doubles_df)\n",
        "    imputed_df = imputer_model.transform(doubles_df)\n",
        "\n",
        "    return imputed_df"
      ],
      "metadata": {
        "id": "G6xIvTl0U0s4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_LinearRegression(imputed_df, labelCol=\"price\"):\n",
        "\n",
        "    train_df, test_df = imputed_df.randomSplit([.8, .2] , seed=42)\n",
        "\n",
        "    with mlflow.start_run(run_name=\"LinearRegression\") as run:\n",
        "\n",
        "        # Define pipeline\n",
        "        categorical_cols = [field for (field, dataType) in train_df.dtypes if dataType == \"string\"]\n",
        "        index_output_cols = [x + \"Index\" for x in categorical_cols]\n",
        "        ohe_output_cols = [x + \"OHE\" for x in categorical_cols]\n",
        "        string_indexer = StringIndexer(inputCols=categorical_cols, outputCols=index_output_cols, handleInvalid=\"skip\")\n",
        "        ohe_encoder = OneHotEncoder(inputCols=index_output_cols, outputCols=ohe_output_cols)\n",
        "        numeric_cols = [field for (field, dataType) in train_df.dtypes if ((dataType == \"double\") & (field != labelCol))]\n",
        "        assembler_inputs = ohe_output_cols + numeric_cols\n",
        "        vec_assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n",
        "\n",
        "        lr = LinearRegression(labelCol=labelCol, featuresCol=\"features\")\n",
        "\n",
        "        stages = [string_indexer, ohe_encoder, vec_assembler, lr]\n",
        "\n",
        "        pipeline = Pipeline(stages=stages)\n",
        "        pipeline_model = pipeline.fit(train_df)\n",
        "\n",
        "        # Log parameters\n",
        "        # mlflow.log_param(\"label\", labelCol)\n",
        "        # mlflow.log_param(\"features\", \"multiple\")\n",
        "\n",
        "        # Evaluate predictions\n",
        "        pred_df = pipeline_model.transform(test_df)\n",
        "        regression_evaluator = RegressionEvaluator(labelCol=labelCol, predictionCol=\"prediction\")\n",
        "        rmse = regression_evaluator.setMetricName(\"rmse\").evaluate(pred_df)\n",
        "        r2 = regression_evaluator.setMetricName(\"r2\").evaluate(pred_df)\n",
        "\n",
        "        # Log both metrics\n",
        "        mlflow.log_metric(\"rmse\", rmse)\n",
        "        mlflow.log_metric(\"r2\", r2)\n",
        "\n",
        "        # Log model\n",
        "        mlflow.spark.log_model(pipeline_model, \"model\", input_example=train_df.limit(5).toPandas())"
      ],
      "metadata": {
        "id": "FxFt6NDPU21O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_RandomForestCV(imputed_df, maxBins=40, labelCol=\"price\"):\n",
        "\n",
        "    train_df, test_df = imputed_df.randomSplit([.8, .2] , seed=42)\n",
        "\n",
        "    with mlflow.start_run(run_name=\"RF-GridSearchCV\") as run:\n",
        "\n",
        "        categorical_cols = [field for (field, dataType) in train_df.dtypes if dataType == \"string\"]\n",
        "        index_output_cols = [x + \"Index\" for x in categorical_cols]\n",
        "\n",
        "        string_indexer = StringIndexer(inputCols=categorical_cols, outputCols=index_output_cols, handleInvalid=\"skip\")\n",
        "\n",
        "        numeric_cols = [field for (field, dataType) in train_df.dtypes if ((dataType == \"double\") & (field != labelCol))]\n",
        "        assembler_inputs = index_output_cols + numeric_cols\n",
        "        vec_assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n",
        "\n",
        "        rf = RandomForestRegressor(labelCol=labelCol, maxBins=maxBins)\n",
        "\n",
        "        param_grid = (ParamGridBuilder()\n",
        "                    .addGrid(rf.maxDepth, [2, 5])\n",
        "                    .addGrid(rf.numTrees, [5, 10])\n",
        "                    .build())\n",
        "\n",
        "        evaluator = RegressionEvaluator(labelCol=labelCol, predictionCol=\"prediction\")\n",
        "\n",
        "        # Pipeline in CV: take much longer time if there are estimators in pipeline, which have to be refitted in every validation\n",
        "\n",
        "    #     stages = [string_indexer, vec_assembler, rf]\n",
        "    #     pipeline = Pipeline(stages=stages)\n",
        "    #     cv = CrossValidator(estimator=pipeline, evaluator=evaluator, estimatorParamMaps=param_grid,\n",
        "    #                         numFolds=3, , parallelism=4, seed=42)\n",
        "    #     cv_model = cv.fit(train_df)\n",
        "\n",
        "        # CV in pipeline: potential risk of data leakage\n",
        "        cv = CrossValidator(estimator=rf, evaluator=evaluator, estimatorParamMaps=param_grid,\n",
        "                        numFolds=10, parallelism=4, seed=42)\n",
        "        stages_with_cv = [string_indexer, vec_assembler, cv]\n",
        "        pipeline = Pipeline(stages=stages_with_cv)\n",
        "        pipeline_model = pipeline.fit(train_df)\n",
        "\n",
        "        # Log parameter\n",
        "        # mlflow.log_param(\"label\", \"price\")\n",
        "        # mlflow.log_param(\"features\", \"all_features\")\n",
        "\n",
        "        # Create predictions and metrics\n",
        "        best_model = pipeline_model.stages[-1].bestModel\n",
        "        best_pipeline_model = Pipeline(stages=[string_indexer, vec_assembler, best_model]).fit(train_df)\n",
        "        pred_df = best_pipeline_model.transform(test_df)\n",
        "        rmse = evaluator.setMetricName(\"rmse\").evaluate(pred_df)\n",
        "        r2 = evaluator.setMetricName(\"r2\").evaluate(pred_df)\n",
        "\n",
        "        # Log both metrics\n",
        "        mlflow.log_metric(\"rmse\", rmse)\n",
        "        mlflow.log_metric(\"r2\", r2)\n",
        "\n",
        "        mlflow.spark.log_model(best_pipeline_model, \"model\", input_example=train_df.limit(5).toPandas())\n",
        "\n",
        "        # Log feature_importance\n",
        "        features_df = pd.DataFrame(list(zip(vec_assembler.getInputCols(), best_model.featureImportances)), columns=[\"feature\", \"importance\"])\n",
        "        features_df = features_df.sort_values(by = 'importance',ascending=False).head(10)\n",
        "        fig, ax = plt.subplots()\n",
        "        features_df.plot(kind='barh', x='feature', y='importance', ax=ax)\n",
        "        mlflow.log_figure(fig, \"feature_importance.png\")"
      ],
      "metadata": {
        "id": "dOU3sWTAU43W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_RandomForest_Hyperopt(imputed_df, maxBins=40, labelCol=\"price\"):\n",
        "\n",
        "    train_df, val_df, test_df = imputed_df.randomSplit([.6, .2, .2], seed=42)\n",
        "\n",
        "    categorical_cols = [field for (field, dataType) in train_df.dtypes if dataType == \"string\"]\n",
        "    index_output_cols = [x + \"Index\" for x in categorical_cols]\n",
        "    string_indexer = StringIndexer(inputCols=categorical_cols, outputCols=index_output_cols, handleInvalid=\"skip\")\n",
        "    numeric_cols = [field for (field, dataType) in train_df.dtypes if ((dataType == \"double\") & (field != labelCol))]\n",
        "    assembler_inputs = index_output_cols + numeric_cols\n",
        "    vec_assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n",
        "\n",
        "    rf = RandomForestRegressor(labelCol=labelCol, maxBins=maxBins)\n",
        "    pipeline = Pipeline(stages=[string_indexer, vec_assembler, rf])\n",
        "    evaluator = RegressionEvaluator(labelCol=labelCol, predictionCol=\"prediction\")\n",
        "\n",
        "\n",
        "    def objective_function(params):\n",
        "        # set the hyperparameters that we want to tune\n",
        "        max_depth = params[\"max_depth\"]\n",
        "        num_trees = params[\"num_trees\"]\n",
        "        with mlflow.start_run():\n",
        "            estimator = pipeline.copy({rf.maxDepth: max_depth, rf.numTrees: num_trees})\n",
        "            model = estimator.fit(train_df)\n",
        "            preds = model.transform(val_df)\n",
        "            rmse = evaluator.evaluate(preds)\n",
        "            #mlflow.log_metric(\"rmse_val\", rmse)\n",
        "        return rmse\n",
        "\n",
        "\n",
        "    search_space = {\n",
        "        \"max_depth\": hp.quniform(\"max_depth\", 2, 5, 1),\n",
        "        \"num_trees\": hp.quniform(\"num_trees\", 10, 100, 1)\n",
        "    }\n",
        "\n",
        "    num_evals = 4\n",
        "    trials = Trials()\n",
        "    best_hyperparam = fmin(fn=objective_function,\n",
        "                        space=search_space,\n",
        "                        algo=tpe.suggest,\n",
        "                        max_evals=num_evals,\n",
        "                        trials=trials,\n",
        "                        rstate=np.random.default_rng(42))\n",
        "\n",
        "\n",
        "    with mlflow.start_run(run_name=\"RF-Hyperopt\") as run:\n",
        "        best_max_depth = best_hyperparam[\"max_depth\"]\n",
        "        best_num_trees = best_hyperparam[\"num_trees\"]\n",
        "        estimator = pipeline.copy({rf.maxDepth: best_max_depth, rf.numTrees: best_num_trees})\n",
        "        combined_df = train_df.union(val_df) # Combine train & validation together\n",
        "\n",
        "        pipeline_model = estimator.fit(combined_df)\n",
        "        pred_df = pipeline_model.transform(test_df)\n",
        "\n",
        "        rmse = evaluator.setMetricName(\"rmse\").evaluate(pred_df)\n",
        "        r2 = evaluator.setMetricName(\"r2\").evaluate(pred_df)\n",
        "\n",
        "        # Log param and metrics for the final model\n",
        "        # mlflow.log_param(\"maxDepth\", best_max_depth)\n",
        "        # mlflow.log_param(\"numTrees\", best_num_trees)\n",
        "        mlflow.log_metric(\"rmse\", rmse)\n",
        "        mlflow.log_metric(\"r2\", r2)\n",
        "\n",
        "        mlflow.spark.log_model(pipeline_model, \"model\", input_example=combined_df.limit(5).toPandas())\n",
        "\n",
        "        best_model = pipeline_model.stages[-1]\n",
        "        features_df = pd.DataFrame(list(zip(vec_assembler.getInputCols(), best_model.featureImportances)), columns=[\"feature\", \"importance\"])\n",
        "        features_df = features_df.sort_values(by = 'importance',ascending=False).head(10)\n",
        "        fig, ax = plt.subplots()\n",
        "        features_df.plot(kind='barh', x='feature', y='importance', ax=ax)\n",
        "        mlflow.log_figure(fig, \"feature_importance.png\")"
      ],
      "metadata": {
        "id": "mNSsGW9IU8BP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pysparkling.ml import H2OAutoML\n",
        "from pysparkling import *\n",
        "\n",
        "\n",
        "def run_H2OAutoML():\n",
        "    spark = get_sparkSession(appName = 'H2OautoML')\n",
        "    imputed_df = clean_impute_dataframe(spark, file_uri, keep_cols, impute_cols, impute_strategy = \"median\")\n",
        "    train_df, test_df = imputed_df.randomSplit([.8, .2] , seed=42)\n",
        "\n",
        "    hc = H2OContext.getOrCreate()\n",
        "\n",
        "    with mlflow.start_run(run_name=\"H2O-autoML\") as run:\n",
        "\n",
        "        automl = H2OAutoML(labelCol=\"price\", convertUnknownCategoricalLevelsToNa=True)\n",
        "        automl.setExcludeAlgos([\"GLM\",\"DeepLearning\"])\n",
        "        automl.setMaxModels(10)\n",
        "        automl.setSortMetric(\"rmse\")\n",
        "\n",
        "        model = automl.fit(train_df)\n",
        "        from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "        pred_df = model.transform(test_df)\n",
        "        regression_evaluator = RegressionEvaluator(labelCol='price', predictionCol=\"prediction\")\n",
        "        rmse = regression_evaluator.evaluate(pred_df)\n",
        "        r2 = regression_evaluator.setMetricName(\"r2\").evaluate(pred_df)\n",
        "\n",
        "\n",
        "        mlflow.log_metric(\"rmse\", rmse)\n",
        "        mlflow.log_metric(\"r2\", r2)\n",
        "        mlflow.spark.log_model(model, 'model')"
      ],
      "metadata": {
        "id": "vd6crTi9U9mN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}